{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "debe1926",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib.pyplot import subplots\n",
    "import statsmodels.api as sm\n",
    "from ISLP import load_data\n",
    "from ISLP.models import (ModelSpec as MS,\n",
    "summarize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd6bfbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ISLP import confusion_table\n",
    "from ISLP.models import contrast\n",
    "from sklearn.discriminant_analysis import \\\n",
    "(LinearDiscriminantAnalysis as LDA,\n",
    "QuadraticDiscriminantAnalysis as QDA)\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7da81f46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of                        Model  Release date  Max resolution  Low resolution  \\\n",
       "0           Agfa ePhoto 1280          1997          1024.0           640.0   \n",
       "1           Agfa ePhoto 1680          1998          1280.0           640.0   \n",
       "2           Agfa ePhoto CL18          2000           640.0             0.0   \n",
       "3           Agfa ePhoto CL30          1999          1152.0           640.0   \n",
       "4     Agfa ePhoto CL30 Clik!          1999          1152.0           640.0   \n",
       "...                      ...           ...             ...             ...   \n",
       "1033         Toshiba PDR-M65          2001          2048.0          1024.0   \n",
       "1034         Toshiba PDR-M70          2000          2048.0          1024.0   \n",
       "1035         Toshiba PDR-M71          2001          2048.0          1024.0   \n",
       "1036         Toshiba PDR-M81          2001          2400.0          1200.0   \n",
       "1037         Toshiba PDR-T10          2002          1600.0           800.0   \n",
       "\n",
       "      Effective pixels  Zoom wide (W)  Zoom tele (T)  Normal focus range  \\\n",
       "0                  0.0           38.0          114.0                70.0   \n",
       "1                  1.0           38.0          114.0                50.0   \n",
       "2                  0.0           45.0           45.0                 0.0   \n",
       "3                  0.0           35.0           35.0                 0.0   \n",
       "4                  0.0           43.0           43.0                50.0   \n",
       "...                ...            ...            ...                 ...   \n",
       "1033               3.0           38.0          114.0                10.0   \n",
       "1034               3.0           35.0          105.0                80.0   \n",
       "1035               3.0           35.0           98.0                80.0   \n",
       "1036               3.0           35.0           98.0                80.0   \n",
       "1037               1.0           38.0           38.0                40.0   \n",
       "\n",
       "      Macro focus range  Storage included  Weight (inc. batteries)  \\\n",
       "0                  40.0               4.0                    420.0   \n",
       "1                   0.0               4.0                    420.0   \n",
       "2                   0.0               2.0                      0.0   \n",
       "3                   0.0               4.0                      0.0   \n",
       "4                   0.0              40.0                    300.0   \n",
       "...                 ...               ...                      ...   \n",
       "1033               10.0               8.0                    320.0   \n",
       "1034                9.0              16.0                    390.0   \n",
       "1035               10.0               8.0                    340.0   \n",
       "1036               10.0              16.0                    340.0   \n",
       "1037               20.0               8.0                    180.0   \n",
       "\n",
       "      Dimensions   Price  \n",
       "0           95.0   179.0  \n",
       "1          158.0   179.0  \n",
       "2            0.0   179.0  \n",
       "3            0.0   269.0  \n",
       "4          128.0  1299.0  \n",
       "...          ...     ...  \n",
       "1033       120.0    62.0  \n",
       "1034       116.0    62.0  \n",
       "1035       107.0    62.0  \n",
       "1036       107.0    62.0  \n",
       "1037        86.0   129.0  \n",
       "\n",
       "[1038 rows x 13 columns]>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "camera = pd.read_csv('camera_dataset.csv')\n",
    "camera.head\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68e5a22c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Model', 'Release date', 'Max resolution', 'Low resolution',\n",
       "       'Effective pixels', 'Zoom wide (W)', 'Zoom tele (T)',\n",
       "       'Normal focus range', 'Macro focus range', 'Storage included',\n",
       "       'Weight (inc. batteries)', 'Dimensions', 'Price'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "camera.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82d1875",
   "metadata": {},
   "source": [
    "adding Prange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "621488e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       medium\n",
      "1       medium\n",
      "2       medium\n",
      "3       medium\n",
      "4         high\n",
      "         ...  \n",
      "1033       low\n",
      "1034       low\n",
      "1035       low\n",
      "1036       low\n",
      "1037       low\n",
      "Name: pRange, Length: 1038, dtype: category\n",
      "Categories (3, object): ['low' < 'medium' < 'high']\n"
     ]
    }
   ],
   "source": [
    "camera['pRange'] = pd.cut(camera['Price'], \n",
    "                             bins=[-float('inf'), 150, 399, float('inf')], \n",
    "                             labels=['low', 'medium', 'high'])\n",
    "print(camera['pRange'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9d66213",
   "metadata": {},
   "outputs": [],
   "source": [
    "camera['Release date'] = pd.to_numeric(camera['Release date'], errors='coerce')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b44b3a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training data\n",
    "train_df = camera[(camera['Release date'] >= 1994) & (camera['Release date'] <= 2004)]\n",
    "#Testing data\n",
    "test_df = camera[(camera['Release date'] >= 2005) & (camera['Release date'] <= 2007)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b471245",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "976db8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = LabelEncoder().fit_transform(train_df['pRange'])\n",
    "y_test = LabelEncoder().fit_transform(test_df['pRange'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f85aa6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df.drop(columns=['Model', 'Price', 'Release date', 'pRange'])\n",
    "X_test = test_df.drop(columns=['Model', 'Price', 'Release date', 'pRange'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "35cb5ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    " lda = LDA(store_covariance=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d3d8c5e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearDiscriminantAnalysis(store_covariance=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearDiscriminantAnalysis</label><div class=\"sk-toggleable__content\"><pre>LinearDiscriminantAnalysis(store_covariance=True)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearDiscriminantAnalysis(store_covariance=True)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6bf9bb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.fillna(X_train.mean(), inplace=True)\n",
    "X_test.fillna(X_test.mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "96366221",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_lda = lda.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3d61d7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1dfe97ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 49,   2,  35],\n",
       "        [ 28,   9, 124],\n",
       "        [ 20,  15, 177]], dtype=int64),\n",
       " 0.4880174291938998)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf_matrix_lda = confusion_matrix(y_test, y_pred_lda)\n",
    "overall_accuracy_lda = accuracy_score(y_test, y_pred_lda)\n",
    "overall_error_rate_lda = 1 - overall_accuracy_lda\n",
    "\n",
    "conf_matrix_lda, overall_error_rate_lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d65bc6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556f763e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6b1b09bf",
   "metadata": {},
   "source": [
    "------LDA---------\n",
    "\n",
    "- **Low Price Range Error Percentage**: (2 + 35) / 86           43.02%\n",
    "- **Medium Price Range Error Percentage**: (28 + 124) / 161     94.41%\n",
    "- **High Price Range Error Percentage**: (20 + 15) / 212        16.51%\n",
    "\n",
    "\n",
    "Overall Error Rate: 48.8%\n",
    "\n",
    "\n",
    "The error seems to be particularly bad within the medium price range, with an error percentage of 94.41%. This indicates that the model struggles significantly with correctly classifying cameras in the medium price range, misclassifying them as either low or high in the vast majority of cases. The high price range sees the least error, suggesting the model is relatively more successful at identifying cameras in this category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7555eba6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>QuadraticDiscriminantAnalysis(store_covariance=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">QuadraticDiscriminantAnalysis</label><div class=\"sk-toggleable__content\"><pre>QuadraticDiscriminantAnalysis(store_covariance=True)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "QuadraticDiscriminantAnalysis(store_covariance=True)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qda = QDA(store_covariance=True)\n",
    "qda.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "12fc9773",
   "metadata": {},
   "outputs": [],
   "source": [
    " qda_pred = qda.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "da68b2b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[50, 19, 17],\n",
       "        [19, 63, 79],\n",
       "        [27, 94, 91]], dtype=int64),\n",
       " 0.5555555555555556)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf_matrix_qda = confusion_matrix(y_test, qda_pred)\n",
    "overall_accuracy_qda = accuracy_score(y_test, qda_pred)\n",
    "overall_error_rate_qda = 1 - overall_accuracy_qda\n",
    "\n",
    "conf_matrix_qda, overall_error_rate_qda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0240b5b",
   "metadata": {},
   "source": [
    "\n",
    "----------QDA------------\n",
    "- **Low Price Range Error Percentage**:  (19+17) / (50+19+17) 41.86%\n",
    "- **Medium Price Range Error Percentage**: (19+79) / (19+63+79) 60.87%\n",
    "- **High Price Range Error Percentage**: (27+94) / (27+94+91) 57.08%\n",
    "\n",
    "\n",
    "OVERALL - 55.6%\n",
    "\n",
    "\n",
    "The QDA model does a bit better than the LDA model, especially when predicting cameras in the medium price range. While the LDA model had a lot of trouble with these medium-priced cameras, the QDA model still struggles but not as much. Both models find it hard to accurately classify cameras by their price ranges, showing that choosing the right model is important, but so are other things like picking the best features and having good data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8b274662",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GaussianNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GaussianNB</label><div class=\"sk-toggleable__content\"><pre>GaussianNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NB = GaussianNB()\n",
    "NB.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a3179938",
   "metadata": {},
   "outputs": [],
   "source": [
    " nb_pred = NB.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7bc12ea2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 67,   2,  17],\n",
       "        [ 58,   2, 101],\n",
       "        [ 93,   1, 118]], dtype=int64),\n",
       " 0.5925925925925926)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf_matrix_nb = confusion_matrix(y_test, nb_pred)\n",
    "overall_accuracy_nb = accuracy_score(y_test, nb_pred)\n",
    "overall_error_rate_nb = 1 - overall_accuracy_nb\n",
    "\n",
    "conf_matrix_nb, overall_error_rate_nb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6dd30fe",
   "metadata": {},
   "source": [
    " - **Low Price Range Error Percentage**:(2 + 17) / 86            22.06%\n",
    "- **Medium Price Range Error Percentage**: (58 + 101) / 161      98.76%\n",
    "- **High Price Range Error Percentage**:(93 + 1) / 212           44.34%\n",
    "\n",
    "\n",
    "OVERALL - 59.26%\n",
    "\n",
    "In the Naive Bayes model, the error is most noticeable in the medium price range, similar to what we observed with the LDA and QDA models. However, the Naive Bayes model demonstrates a particularly high error rate in this category, suggesting it also struggles significantly with medium-priced cameras. Compared to the LDA and QDA models, the pattern of difficulty with the medium price range persists across all models, indicating a consistent challenge in accurately classifying cameras in this price bracket. This shows that regardless of the model used, predicting the medium price range accurately remains a tough problem, highlighting potential issues with the features used for modeling or the inherent complexity of the data in this specific price range.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5128af",
   "metadata": {},
   "source": [
    "\n",
    "For the camera dataset, Naive Bayes might not be ideal because it assumes features like megapixels and zoom don't affect each other, which is likely not true here. LDA could be better if camera features across different price ranges behave similarly, but if these features vary significantly by price range, QDA might be the best since it allows for such differences. However, if our camera features vary widely by price range, theoretically, QDA should have been the most accurate. The real measure of success, though, depends on how well each model's assumptions align with our specific dataset and whether the model that theoretically fits the best actually delivered the best performance based on our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c57fe84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
